{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfeafde9",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56511e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7613, 5)\n",
      "Test:  (3263, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_fp = os.path.join(os.getcwd(), \"data\")\n",
    "train_fp = os.path.join(data_fp, \"train.csv\")\n",
    "train_full = pd.read_csv(train_fp, encoding=\"utf-8\")\n",
    "test_fp = os.path.join(data_fp, \"test.csv\")\n",
    "test = pd.read_csv(test_fp, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Train: {train_full.shape}\")\n",
    "print(f\"Test:  {test.shape}\")\n",
    "\n",
    "train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2c4d3",
   "metadata": {},
   "source": [
    "Kaggle supplies the test set (without target labels), so we will submit our final predictions there to get the ultimate measure of model performance. We separate 20% of the training data as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336d1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the index in case there are patterns in the order of the training data:\n",
    "np.random.seed(42)\n",
    "shuffled_ix = list(train_full.index)\n",
    "np.random.shuffle(shuffled_ix)\n",
    "cutoff = int(len(shuffled_ix)*0.8)\n",
    "train = train_full.loc[shuffled_ix[:cutoff]].copy()\n",
    "validation = train_full.loc[shuffled_ix[cutoff:]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fe8a7",
   "metadata": {},
   "source": [
    "## Base Model\n",
    "\n",
    "We use CountVectorizer and BernoulliNB to create a baseline model, and get benchmark score against the validation set.\n",
    "\n",
    "Note that the target labels are binary outcomes, so they comrpise a Bernoulli distribution where $p \\approx 0.43$. Therefore accuracy is the chosen metric of performance, since there isn't a huge class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077f2b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli distribution, p = 0.430\n",
      "Mean accuracy on 5-fold cross validation = 0.802\n",
      "Validation accuracy = 0.793\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(train[\"text\"])\n",
    "train_arr = cv.transform(train[\"text\"])\n",
    "train_labels = train[\"target\"]\n",
    "print(f\"Bernoulli distribution, p = {train_labels.mean():.3f}\")\n",
    "model = BernoulliNB()\n",
    "crossval = cross_validate(model, train_arr, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "print(f\"Mean accuracy on 5-fold cross validation = {mean_score:.3f}\")\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "val_arr = cv.transform(validation[\"text\"])\n",
    "val_labels = validation[\"target\"]\n",
    "val_score = best_estimator.score(val_arr, val_labels)\n",
    "print(f\"Validation accuracy = {val_score:.3f}\")\n",
    "cm = plot_confusion_matrix(best_estimator, val_arr, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d52ac",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "To improve performance we explore 2 different avenues:\n",
    "\n",
    "1. How the text data is represented. Specifically we use word embeddings.\n",
    "2. What type of model is used to learn the relationships between embeddings and the target feature.\n",
    "\n",
    "Specifically for part 1, the following word embedding schemes are explored:\n",
    "\n",
    "1. Word2Vec - pretrained\n",
    "2. Word2Vec - corpus trained\n",
    "\n",
    "For part 2, based on the results of <a href=\"https://github.com/MIDS-W207/coursework/blob/master/Readings/Week%2008/An%20Empirical%20Comparison%20of%20Supervised%20Learning%20Algorithms.pdf\">this</a> paper comparing the performance of classifiers, we try out the following 3 classifiers:\n",
    "\n",
    "1. Boosted Trees\n",
    "2. Support Vector Machines\n",
    "3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1dfcd",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Since we are using Word Embeddings, we'll need to standardize the format of the input tweets. From the creators of the dataset there is a <a href=\"https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\">Ruby script</a>, which has been converted to <a href=\"https://www.kaggle.com/amackcrane/python-version-of-glove-twitter-preprocess-script\">Python</a> by a Kaggle user. The below method is adapted from this Python version, with some additional changes to improve the number of input words which can be embedded. Some of these changes include:\n",
    "\n",
    "1. Removing most punctuation entirely. The original Kaggle script leaves a lot of punctation untouched, so that for example the quoted word `'something'` would be included as a separate feature to the unquoted word `something`. We make the assumption that such differences in punctuation of features are unlikely to be significant in identifying disaster tweets.\n",
    "2. Fixing a couple of bugs with the way whitespace was added around certain punctuation marks, such as `.,`\n",
    "3. Add spaces around inserted tags such as ` <number> `\n",
    "\n",
    "Making the above changes to the preprocessor we increase the number of input words which are found in the embedding vocabulary from ~75% to nearly 90%.\n",
    "\n",
    "One interesting decision in the preprocessing script is to remove tagged twitter usernames (i.e. strings starting with `@`), since you might expect that this could provide some useful information in trying to predict the nature of the tweets. For example, a tweet tagging a local fire department might be much more likely to be a disaster related tweet. However on searching the word embedding features for well known Twitter usernames (e.g. `@realdonaldtrump`) we find that they are not present, so this seems to accord with the processing done for the Glove word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f59a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import sys\n",
    "import regex as re\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "\n",
    "punctuation = string.punctuation.replace(\"#\", \"\")\n",
    "punctuation = punctuation.replace(\"<\", \"\")\n",
    "punctuation = punctuation.replace(\">\", \"\")\n",
    "\n",
    "\n",
    "def _hashtag(text):\n",
    "    \"\"\"Parse hashtags in tweets.\n",
    "    \n",
    "    For hashtags with words in title case, splits into separate words, e.g.\n",
    "        #BigFire -> <hashtag> Big Fire \n",
    "        \n",
    "    For all caps hashtags converts to lowercase, e.g.\n",
    "        #OMG -> <hashtag> omg\n",
    "    \"\"\"\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = f\" <hashtag> {hashtag_body.lower()} <allcaps> \"\n",
    "    else:\n",
    "        result = \" \".join([\" <hashtag> \"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _allcaps(text):\n",
    "    \"\"\"Convert all-caps words to lowercase and add allcaps tag.\"\"\"\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps> \"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    # Remove URLS:\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" <url> \")\n",
    "    \n",
    "    # Remove usernames:\n",
    "    text = re_sub(r\"@\\w+\", \" <user> \")\n",
    "    \n",
    "    # Tag smiley faces :-) sad faces :-( etc.\n",
    "    eyes, nose = r\"[8:=;]\", r\"['`\\-]?\"\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \" <smile> \")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \" <lolface> \")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \" <sadface> \")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \" <neutralface> \")\n",
    "    text = re_sub(r\"<3\",\" <heart> \")\n",
    "\n",
    "    # Add whitespace around /\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    \n",
    "    # Tag numbers:\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" <number> \")\n",
    "    \n",
    "    # Tag hastags and separate hashtag words:\n",
    "    text = re_sub(r\"#\\w+\", _hashtag)\n",
    "    \n",
    "    # Remove and tag repetitions of question marks, exclamations:\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat> \")\n",
    "    \n",
    "    # Tag elongated sequences of the same letter (3+ occurences), e.g. \"AAAAAAA\"\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong> \")\n",
    "    \n",
    "    # Put whitespace around punctuation, then remove:\n",
    "    text = re_sub(r\"([a-zA-Z<>()])([{}])\".format(punctuation), r\"\\1 \\2\")\n",
    "    text = re_sub(r\"([{}])([a-zA-Z<>()])\".format(punctuation), r\"\\1 \\2\")\n",
    "    text = re.sub(\"[%s]\" % re.escape(punctuation), \"\", text)\n",
    "\n",
    "    # Tag all caps words:\n",
    "    text = re_sub(r\" ([A-Z]){2,} \", _allcaps)\n",
    "    \n",
    "    return \" \".join(text.lower().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140b6c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<hashtag> <elong> big fire on <hashtag> <elong> mount everest <hashtag> omg <allcaps> <repeat> where are police some punctuation <number> now <number> <user>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example with lots of weird features:\n",
    "text = \"#BigFire on #MountEverest #OMG, !! /where are police 'some' punctuation! 3now 44 @police\"\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1b64b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet:\n",
      "   Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "\n",
      "Kaggle user script:\n",
      " our deeds are the reason of this <hashtag> earthquake may allah <allcaps> forgive us all\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example from the dataset:\n",
    "example_tweet = train[\"text\"][0]\n",
    "print(\"Original Tweet:\\n\", f\"  {example_tweet}\\n\")\n",
    "print(\"Kaggle user script:\\n\", f\"{tokenize(example_tweet)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef5757",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "We use word embeddings to convert each word to a $d$-length vector. Because the models we are using require a single vector input, we need a way to then convert the multiple word embeddings for a single tweet into one vector. Using the suggestion <a href=\"https://stats.stackexchange.com/a/239071/115143\">here</a>, we try 2 approaches:\n",
    "\n",
    "1. Take the simple average of the vectors for all words in the tweet (resulting in vector of length $d$).\n",
    "2. Concatenate the coordinate-wise minimum and maximum values in each word vector (resulting in vector of length $2d$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d80fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spellchecker import SpellChecker\n",
    "import yaml\n",
    "\n",
    "\n",
    "class WordEmbeddings:\n",
    "    \"\"\"Class to create word embeddings from input tweets, \n",
    "    including text pre-processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, X: pd.Series, embeddings: KeyedVectors, \n",
    "                 processor = tokenize, spellcheck: bool = True, \n",
    "                 idf_filter: float = 0.05, stop_words: list = None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Save the raw input as a list:\n",
    "        self.X_raw = X.tolist()\n",
    "        \n",
    "        # Apply the preprocessor function:\n",
    "        X = X.map(processor).tolist()\n",
    "        \n",
    "        # Apply the spelling corrections:\n",
    "        if spellcheck:\n",
    "            corrections = self.correct_spellings([])\n",
    "            corrected = list()\n",
    "            for tweet in X:\n",
    "                corrected.append(\" \".join([corrections.get(w, w) for w in tweet.split()]))\n",
    "            X = corrected\n",
    "        \n",
    "        # Apply IDF to remove common words in corpus:\n",
    "        if idf_filter is not None:\n",
    "            vect = TfidfVectorizer(\n",
    "                input=\"content\", encoding=\"utf-8\", lowercase=True, \n",
    "                analyzer=\"word\", ngram_range=(1, 1), max_df=1.0, min_df=1\n",
    "            )\n",
    "            vect.fit(X)\n",
    "            df = pd.DataFrame({\"word\": vect.get_feature_names(), \"idf\": vect.idf_})\n",
    "            df.sort_values(by=[\"idf\"], ascending=True, inplace=True)\n",
    "            number = int(len(df) * idf_filter)\n",
    "            drop_words = set(df.iloc[:number][\"word\"].tolist())\n",
    "            filtered = list()\n",
    "            for tweet in X:\n",
    "                filtered.append(\" \".join(list(filter(lambda w: w not in drop_words, tweet.split()))))\n",
    "            X = filtered\n",
    "        \n",
    "        # Drop stop words:\n",
    "        if stop_words is not None:\n",
    "            filtered = list()\n",
    "            for tweet in X:\n",
    "                filtered.append(\" \".join(list(filter(lambda w: w not in stop_words, tweet.split()))))\n",
    "            X = filtered\n",
    "\n",
    "        self.X = X\n",
    "        \n",
    "        # Store the word embeddings scheme:\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        # Unique vocabulary of the embedding scheme:\n",
    "        self.embeddings_vocab = set(embeddings.key_to_index)\n",
    "        \n",
    "        # Unique vocabulary of the inputs:\n",
    "        self.X_vocab = {w for words in [s.split() for s in X] for w in words}\n",
    "        \n",
    "        # Unknown words in the input:\n",
    "        self.unknown_words = self.X_vocab - self.embeddings_vocab\n",
    "\n",
    "        # Common vocabulary between the input and the embeddings scheme:\n",
    "        self.common_vocab = self.embeddings_vocab & self.X_vocab\n",
    "        self.vocab_coverage = len(self.common_vocab) / len(self.X_vocab)\n",
    "        print(f\"{len(self.X_vocab):} processed input words, \"\\\n",
    "              f\"{len(self.common_vocab):} found in embeddings \"\\\n",
    "              f\"({self.vocab_coverage*100:.2f}%).\")\n",
    "\n",
    "        # Lookup table from words to their embeddings:\n",
    "        self.embeddings_table = pd.DataFrame(embeddings[self.common_vocab], index=self.common_vocab).sort_index()\n",
    "        \n",
    "        # Iterate through input tweets removing unknown words, creating average/min/max embeddings:\n",
    "        embedding_mu, embedding_max, embedding_min, clean_tweets = list(), list(), list(), list()\n",
    "        for i, tweet in enumerate(self.X):\n",
    "            \n",
    "            # Remove unknown words:\n",
    "            words = tweet.split()\n",
    "            known_words = [w for w in words if w in self.common_vocab]\n",
    "            clean_tweets.append(\" \".join(known_words))\n",
    "            \n",
    "            # Calculate mean of all word embeddings in tweet:\n",
    "            mean_array = self.embeddings_table.loc[known_words].mean().values\n",
    "            embedding_mu.append(mean_array)\n",
    "            \n",
    "            # Calculate min/max of all word embeddings in tweet:\n",
    "            max_array = self.embeddings_table.loc[known_words].max().values\n",
    "            embedding_max.append(max_array)\n",
    "            min_array = self.embeddings_table.loc[known_words].min().values\n",
    "            embedding_min.append(min_array)\n",
    "            \n",
    "        # Arrays of final X datasets in word embedding formats:\n",
    "        self.embedding_mu = pd.DataFrame(np.array(embedding_mu))\n",
    "        self.embedding_min = np.array(embedding_min)\n",
    "        self.embedding_max = np.array(embedding_max)\n",
    "        self.embedding_minmax = pd.DataFrame(np.concatenate([self.embedding_min, self.embedding_max], axis=1))\n",
    "        \n",
    "    @property\n",
    "    def embedding_chars(self):\n",
    "        \"\"\"Unique characters in the word embeddings.\"\"\"\n",
    "        return \"\".join(sorted(set([c for w in self.embeddings.index_to_key for c in w])))\n",
    "    \n",
    "    @staticmethod\n",
    "    def correct_spellings(words: list):\n",
    "        \"\"\"Use pyspellchecker to save a dictionary of unknown words\n",
    "        to corrected spellings, or just load presaved corrections. \n",
    "        See:\n",
    "            https://github.com/barrust/pyspellchecker\n",
    "        \"\"\"\n",
    "        fp = os.path.join(os.getcwd(), \"spelling_corrections.yaml\")\n",
    "        if not os.path.exists(fp):\n",
    "            corrections = dict()\n",
    "            with open(fp, \"w\") as stream:\n",
    "                yaml.safe_dump(corrections, stream)\n",
    "        else:\n",
    "            with open(fp, \"r\") as stream:\n",
    "                corrections = yaml.safe_load(stream)\n",
    "            if corrections is None:\n",
    "                corrections = dict()\n",
    "        to_correct = [w for w in words if w not in corrections]\n",
    "        if to_correct:\n",
    "            spell = SpellChecker()\n",
    "            new = {w: spell.correction(w) for w in to_correct}\n",
    "            corrections = {**corrections, **new}\n",
    "            with open(fp, \"w\") as stream:\n",
    "                yaml.safe_dump(corrections, stream)\n",
    "            print(f\"Saved {len(new):} spelling corrections to file.\")\n",
    "        return corrections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b08b8",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Using a pretrained Twitter Word2Vec model with ~1.2m word vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac983dcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n"
     ]
    }
   ],
   "source": [
    "from gensim import downloader\n",
    "\n",
    "\n",
    "# Load the pretrained word embeddings (this takes a while):\n",
    "glove_twitter = downloader.load(\"glove-twitter-200\")\n",
    "info = downloader.info()\n",
    "print(info[\"models\"][\"glove-twitter-200\"][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a7c11a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12145 processed input words, 11404 found in embeddings (93.90%).\n",
      "5507 processed input words, 5141 found in embeddings (93.35%).\n"
     ]
    }
   ],
   "source": [
    "# Use the WordEmbeddings class to apply embeddings to the training & validation data:\n",
    "# train_we = WordEmbeddings(train[\"text\"], glove_twitter, idf_filter=0.05)\n",
    "# validation_we = WordEmbeddings(validation[\"text\"], glove_twitter, idf_filter=0.05)\n",
    "train_we = WordEmbeddings(train[\"text\"], glove_twitter, idf_filter=None)\n",
    "validation_we = WordEmbeddings(validation[\"text\"], glove_twitter, idf_filter=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89f462",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Using TensorFlow to train model with \"early stopping\" to prevent over-fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c09a37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TfModel:\n",
    "    \n",
    "    def __init__(self, X_train: WordEmbeddings, y_train: pd.Series, \n",
    "                 X_val: WordEmbeddings, y_val: pd.Series, \n",
    "                 epochs: int = 500, patience: int = 5):\n",
    "        \"\"\"Implement a TensorFlow model on WordEmbeddings training and \n",
    "        validation data.\"\"\"\n",
    "        \n",
    "        # Store model inputs:\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "                \n",
    "        # Train separate models for different embeddings schemes:\n",
    "        self.results, self.best_score, self.best_model = dict(), -np.inf, None\n",
    "        for embeddings in (\"embedding_mu\", \"embedding_minmax\"):            \n",
    "            print(f\"Training {embeddings} model\\n\")\n",
    "            \n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(2)\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=\"adam\",\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[\"accuracy\"]\n",
    "            )\n",
    "\n",
    "            callback = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_accuracy\", \n",
    "                mode=\"max\", \n",
    "                min_delta=0.001,\n",
    "                patience=patience, \n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            X_train_df = getattr(X_train, embeddings)\n",
    "            X_val_df = getattr(X_val, embeddings)\n",
    "            history = model.fit(\n",
    "                x=X_train_df,\n",
    "                y=self.y_train,\n",
    "                batch_size=512,\n",
    "                callbacks=[callback],\n",
    "                verbose=2,\n",
    "                validation_data=(X_val_df, self.y_val),\n",
    "                epochs=epochs,\n",
    "            )\n",
    "\n",
    "            # Use probabilities to make predictions on validation set:\n",
    "            proba_model = tf.keras.Sequential([\n",
    "                model, \n",
    "                tf.keras.layers.Softmax()\n",
    "            ])\n",
    "            train_proba_pred = proba_model.predict(X_train_df)\n",
    "            val_proba_pred = proba_model.predict(X_val_df)\n",
    "            train_pred = np.argmax(train_proba_pred, axis=1)\n",
    "            val_pred = np.argmax(val_proba_pred, axis=1)\n",
    "            \n",
    "            # Store full results:\n",
    "            results_data = {\n",
    "                \"model\": model,\n",
    "                \"history\": history,\n",
    "                \"proba_model\": proba_model,\n",
    "                \"train_proba_pred\": train_proba_pred,\n",
    "                \"val_proba_pred\": val_proba_pred,\n",
    "                \"train_pred\": train_pred,\n",
    "                \"val_pred\": val_pred,\n",
    "            }\n",
    "            self.results[embeddings] = results_data\n",
    "            \n",
    "            # Score the model on overall accuracy on validation set:\n",
    "            accuracy = accuracy_score(self.y_val, val_pred)\n",
    "            if accuracy > self.best_score:\n",
    "                self.best_score = accuracy\n",
    "                self.best_model = embeddings \n",
    "    \n",
    "    @property\n",
    "    def inspect_train(self):\n",
    "        df = pd.DataFrame({\n",
    "            \"raw_tweet\": self.X_train.X_raw,\n",
    "            \"processed_tweet\": self.X_train.X,\n",
    "            \"true_label\": self.y_train.values,\n",
    "            \"prediction\": self.results[self.best_model][\"train_pred\"]\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    @property\n",
    "    def inspect_val(self):\n",
    "        df = pd.DataFrame({\n",
    "            \"raw_tweet\": self.X_val.X_raw,\n",
    "            \"processed_tweet\": self.X_val.X,\n",
    "            \"true_label\": self.y_val.values,\n",
    "            \"prediction\": self.results[self.best_model][\"val_pred\"]\n",
    "        })\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fbc5bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training embedding_mu model\n",
      "\n",
      "Epoch 1/500\n",
      "12/12 - 0s - loss: 0.6134 - accuracy: 0.6732 - val_loss: 0.5442 - val_accuracy: 0.7380\n",
      "Epoch 2/500\n",
      "12/12 - 0s - loss: 0.5089 - accuracy: 0.7632 - val_loss: 0.4948 - val_accuracy: 0.7577\n",
      "Epoch 3/500\n",
      "12/12 - 0s - loss: 0.4637 - accuracy: 0.7903 - val_loss: 0.4714 - val_accuracy: 0.7728\n",
      "Epoch 4/500\n",
      "12/12 - 0s - loss: 0.4406 - accuracy: 0.8031 - val_loss: 0.4620 - val_accuracy: 0.7735\n",
      "Epoch 5/500\n",
      "12/12 - 0s - loss: 0.4258 - accuracy: 0.8125 - val_loss: 0.4585 - val_accuracy: 0.7827\n",
      "Epoch 6/500\n",
      "12/12 - 0s - loss: 0.4158 - accuracy: 0.8181 - val_loss: 0.4555 - val_accuracy: 0.7853\n",
      "Epoch 7/500\n",
      "12/12 - 0s - loss: 0.4074 - accuracy: 0.8259 - val_loss: 0.4539 - val_accuracy: 0.7840\n",
      "Epoch 8/500\n",
      "12/12 - 0s - loss: 0.4003 - accuracy: 0.8291 - val_loss: 0.4525 - val_accuracy: 0.7814\n",
      "Epoch 9/500\n",
      "12/12 - 0s - loss: 0.3948 - accuracy: 0.8287 - val_loss: 0.4525 - val_accuracy: 0.7932\n",
      "Epoch 10/500\n",
      "12/12 - 0s - loss: 0.3863 - accuracy: 0.8383 - val_loss: 0.4561 - val_accuracy: 0.7859\n",
      "Epoch 11/500\n",
      "12/12 - 0s - loss: 0.3787 - accuracy: 0.8401 - val_loss: 0.4541 - val_accuracy: 0.7873\n",
      "Epoch 12/500\n",
      "12/12 - 0s - loss: 0.3708 - accuracy: 0.8433 - val_loss: 0.4561 - val_accuracy: 0.7827\n",
      "Epoch 13/500\n",
      "12/12 - 0s - loss: 0.3639 - accuracy: 0.8442 - val_loss: 0.4587 - val_accuracy: 0.7859\n",
      "Epoch 14/500\n",
      "12/12 - 0s - loss: 0.3552 - accuracy: 0.8481 - val_loss: 0.4607 - val_accuracy: 0.7866\n",
      "Epoch 15/500\n",
      "12/12 - 0s - loss: 0.3472 - accuracy: 0.8552 - val_loss: 0.4744 - val_accuracy: 0.7919\n",
      "Epoch 16/500\n",
      "12/12 - 0s - loss: 0.3435 - accuracy: 0.8573 - val_loss: 0.4686 - val_accuracy: 0.7827\n",
      "Epoch 17/500\n",
      "12/12 - 0s - loss: 0.3328 - accuracy: 0.8581 - val_loss: 0.4703 - val_accuracy: 0.7905\n",
      "Epoch 18/500\n",
      "12/12 - 0s - loss: 0.3244 - accuracy: 0.8637 - val_loss: 0.4708 - val_accuracy: 0.7899\n",
      "Epoch 19/500\n",
      "12/12 - 0s - loss: 0.3134 - accuracy: 0.8691 - val_loss: 0.4775 - val_accuracy: 0.7820\n",
      "Training embedding_minmax model\n",
      "\n",
      "Epoch 1/500\n",
      "12/12 - 0s - loss: 0.7019 - accuracy: 0.5657 - val_loss: 0.6465 - val_accuracy: 0.6310\n",
      "Epoch 2/500\n",
      "12/12 - 0s - loss: 0.6066 - accuracy: 0.6877 - val_loss: 0.5712 - val_accuracy: 0.7157\n",
      "Epoch 3/500\n",
      "12/12 - 0s - loss: 0.5495 - accuracy: 0.7429 - val_loss: 0.5265 - val_accuracy: 0.7492\n",
      "Epoch 4/500\n",
      "12/12 - 0s - loss: 0.5058 - accuracy: 0.7626 - val_loss: 0.5179 - val_accuracy: 0.7393\n",
      "Epoch 5/500\n",
      "12/12 - 0s - loss: 0.4794 - accuracy: 0.7791 - val_loss: 0.4989 - val_accuracy: 0.7571\n",
      "Epoch 6/500\n",
      "12/12 - 0s - loss: 0.4604 - accuracy: 0.7918 - val_loss: 0.4803 - val_accuracy: 0.7748\n",
      "Epoch 7/500\n",
      "12/12 - 0s - loss: 0.4542 - accuracy: 0.7954 - val_loss: 0.4723 - val_accuracy: 0.7741\n",
      "Epoch 8/500\n",
      "12/12 - 0s - loss: 0.4424 - accuracy: 0.8013 - val_loss: 0.4694 - val_accuracy: 0.7735\n",
      "Epoch 9/500\n",
      "12/12 - 0s - loss: 0.4360 - accuracy: 0.8069 - val_loss: 0.4718 - val_accuracy: 0.7748\n",
      "Epoch 10/500\n",
      "12/12 - 0s - loss: 0.4333 - accuracy: 0.8110 - val_loss: 0.4668 - val_accuracy: 0.7774\n",
      "Epoch 11/500\n",
      "12/12 - 0s - loss: 0.4245 - accuracy: 0.8148 - val_loss: 0.4672 - val_accuracy: 0.7794\n",
      "Epoch 12/500\n",
      "12/12 - 0s - loss: 0.4325 - accuracy: 0.8064 - val_loss: 0.4642 - val_accuracy: 0.7715\n",
      "Epoch 13/500\n",
      "12/12 - 0s - loss: 0.4243 - accuracy: 0.8146 - val_loss: 0.4632 - val_accuracy: 0.7781\n",
      "Epoch 14/500\n",
      "12/12 - 0s - loss: 0.4150 - accuracy: 0.8213 - val_loss: 0.4638 - val_accuracy: 0.7761\n",
      "Epoch 15/500\n",
      "12/12 - 0s - loss: 0.4185 - accuracy: 0.8166 - val_loss: 0.4668 - val_accuracy: 0.7761\n",
      "Epoch 16/500\n",
      "12/12 - 0s - loss: 0.4121 - accuracy: 0.8246 - val_loss: 0.4734 - val_accuracy: 0.7774\n",
      "Epoch 17/500\n",
      "12/12 - 0s - loss: 0.4049 - accuracy: 0.8259 - val_loss: 0.4649 - val_accuracy: 0.7735\n",
      "Epoch 18/500\n",
      "12/12 - 0s - loss: 0.4010 - accuracy: 0.8263 - val_loss: 0.4664 - val_accuracy: 0.7768\n",
      "Epoch 19/500\n",
      "12/12 - 0s - loss: 0.4023 - accuracy: 0.8256 - val_loss: 0.4673 - val_accuracy: 0.7748\n",
      "Epoch 20/500\n",
      "12/12 - 0s - loss: 0.3999 - accuracy: 0.8271 - val_loss: 0.4740 - val_accuracy: 0.7748\n",
      "Epoch 21/500\n",
      "12/12 - 0s - loss: 0.3957 - accuracy: 0.8309 - val_loss: 0.4676 - val_accuracy: 0.7728\n"
     ]
    }
   ],
   "source": [
    "tf_nn = TfModel(\n",
    "    train_we, train_labels, validation_we, val_labels, epochs=500, patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfc2ae50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7931713722915299"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_nn.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90e123e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embedding_mu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_nn.best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa92d75",
   "metadata": {},
   "source": [
    "Inspect some False Positives / False Negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0be87f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = tf_nn.inspect_val[\n",
    "    (tf_nn.inspect_val[\"true_label\"] == 0) & \n",
    "    (tf_nn.inspect_val[\"prediction\"] == 1)\n",
    "]\n",
    "false_negatives = tf_nn.inspect_val[\n",
    "    (tf_nn.inspect_val[\"true_label\"] == 1) & \n",
    "    (tf_nn.inspect_val[\"prediction\"] == 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7e8fa8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>true_label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Practice your families fire escape plan so everyone knows what to do in case of an emergency.</td>\n",
       "      <td>practice your families fire escape plan so everyone knows what to do in case of an emergency</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ykelquiban: Breaking news! Unconfirmed! I just heard a loud bang nearby. in what appears to be a blast of wind from my neighbour's ass.</td>\n",
       "      <td>ykelquiban breaking news unconfirmed i just heard a loud bang nearby in what appears to be a blast of wind from my neighbour s ass</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube</td>\n",
       "      <td>the five fatal flaws in the iran deal &lt;url&gt; via &lt;user&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>@comcastcares hey it's happing again. Any trouble shooting steps for when this happens?</td>\n",
       "      <td>&lt;user&gt; hey it s happing again any trouble shooting steps for when this happens</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>cancel the fucking show. Evacuate MetLife  https://t.co/SkQ8oUcM3R</td>\n",
       "      <td>cancel the fucking show evacuate metlife &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>EMS1: NY EMTs petition for $17 per hour Û÷minimum wageÛª http://t.co/4oa6SWlxmR #ems #paramedics #ambulance</td>\n",
       "      <td>ems &lt;number&gt; ny &lt;allcaps&gt; emts petition for &lt;number&gt; per hour û÷minimum wageûª &lt;url&gt; &lt;hashtag&gt; ems &lt;hashtag&gt; paramedics &lt;hashtag&gt; ambulance</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>The Danger and Excitement of Underwater Cave Diving http://t.co/8c3fPloxcr http://t.co/cBGZ9xuN2k</td>\n",
       "      <td>the danger and excitement of underwater cave diving &lt;url&gt; &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>@CaraJDeIevingnc the bomb impact ratio hit beyond kyle js</td>\n",
       "      <td>&lt;user&gt; the bomb impact ratio hit beyond kyle js</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>How many cars do those cyclists wreck going past scratching with pedals. They should be banned #c4news</td>\n",
       "      <td>how many cars do those cyclists wreck going past scratching with pedals they should be banned &lt;hashtag&gt; c &lt;number&gt; news</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>Fire waves and darkness</td>\n",
       "      <td>fire waves and darkness</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    raw_tweet  \\\n",
       "6                                               Practice your families fire escape plan so everyone knows what to do in case of an emergency.   \n",
       "8     ykelquiban: Breaking news! Unconfirmed! I just heard a loud bang nearby. in what appears to be a blast of wind from my neighbour's ass.   \n",
       "11                                                                 The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube   \n",
       "39                                                    @comcastcares hey it's happing again. Any trouble shooting steps for when this happens?   \n",
       "43                                                                         cancel the fucking show. Evacuate MetLife  https://t.co/SkQ8oUcM3R   \n",
       "...                                                                                                                                       ...   \n",
       "1478                            EMS1: NY EMTs petition for $17 per hour Û÷minimum wageÛª http://t.co/4oa6SWlxmR #ems #paramedics #ambulance   \n",
       "1482                                        The Danger and Excitement of Underwater Cave Diving http://t.co/8c3fPloxcr http://t.co/cBGZ9xuN2k   \n",
       "1484                                                                                @CaraJDeIevingnc the bomb impact ratio hit beyond kyle js   \n",
       "1502                                   How many cars do those cyclists wreck going past scratching with pedals. They should be banned #c4news   \n",
       "1516                                                                                                                  Fire waves and darkness   \n",
       "\n",
       "                                                                                                                                    processed_tweet  \\\n",
       "6                                                      practice your families fire escape plan so everyone knows what to do in case of an emergency   \n",
       "8                ykelquiban breaking news unconfirmed i just heard a loud bang nearby in what appears to be a blast of wind from my neighbour s ass   \n",
       "11                                                                                           the five fatal flaws in the iran deal <url> via <user>   \n",
       "39                                                                   <user> hey it s happing again any trouble shooting steps for when this happens   \n",
       "43                                                                                                   cancel the fucking show evacuate metlife <url>   \n",
       "...                                                                                                                                             ...   \n",
       "1478  ems <number> ny <allcaps> emts petition for <number> per hour û÷minimum wageûª <url> <hashtag> ems <hashtag> paramedics <hashtag> ambulance   \n",
       "1482                                                                                the danger and excitement of underwater cave diving <url> <url>   \n",
       "1484                                                                                                <user> the bomb impact ratio hit beyond kyle js   \n",
       "1502                        how many cars do those cyclists wreck going past scratching with pedals they should be banned <hashtag> c <number> news   \n",
       "1516                                                                                                                        fire waves and darkness   \n",
       "\n",
       "      true_label  prediction  \n",
       "6              0           1  \n",
       "8              0           1  \n",
       "11             0           1  \n",
       "39             0           1  \n",
       "43             0           1  \n",
       "...          ...         ...  \n",
       "1478           0           1  \n",
       "1482           0           1  \n",
       "1484           0           1  \n",
       "1502           0           1  \n",
       "1516           0           1  \n",
       "\n",
       "[123 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d858d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>true_label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp;amp; Catch Fire on Amazon. :D</td>\n",
       "      <td>&lt;user&gt; one good thing came out of watching the film was too traumatised to watch show so started halt amp catch fire on amazon &lt;smile&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year</td>\n",
       "      <td>gonna call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>@TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.</td>\n",
       "      <td>&lt;user&gt; it s the closest structure to the hypo centre that wasn t completely obliterated</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died</td>\n",
       "      <td>oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv</td>\n",
       "      <td>bigamist and his first wife are charged in the deaths of his second pregnant wife her child &lt;number&gt; her mothe &lt;repeat&gt; &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>See Aug 4 2015 PoconoRecord @EmergencyMgtMag - How Many Households Have an #Emergency Plan? | http://t.co/7zlsUmIess http://t.co/TdccH01N7q</td>\n",
       "      <td>see aug &lt;number&gt; &lt;number&gt; poconorecord &lt;user&gt; how many households have an &lt;hashtag&gt; &lt;elong&gt; emergency plan &lt;url&gt; &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>Pandemonium In Aba As Woman Delivers Baby Without Face (Photos) - http://t.co/xRP0rTkFfJ</td>\n",
       "      <td>pandemonium in aba as woman delivers baby without face photos &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>#volleyball Attack II Volleyball Training Machine - Sets Simulation - http://t.co/dCDeCFv934 http://t.co/dWBC1dUvdk</td>\n",
       "      <td>&lt;hashtag&gt; volleyball attack ii &lt;allcaps&gt; volleyball training machine sets simulation &lt;url&gt; &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>kesabaran membuahkan hasil indah pada saat tepat! life isn't about waiting for the storm to pass it's about learning to dance in the rain.</td>\n",
       "      <td>kesabaran membuahkan hasil indah pada saat tepat life isn t about waiting for the storm to pass it s about learning to dance in the rain</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>I moved to England five years ago today. What a whirlwind of time it has been! http://t.co/eaSlGeA1B7</td>\n",
       "      <td>i moved to england five years ago today what a whirlwind of time it has been &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           raw_tweet  \\\n",
       "5     @PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp; Catch Fire on Amazon. :D   \n",
       "22             Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year   \n",
       "38                                                  @TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.   \n",
       "55                               oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died   \n",
       "72             Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv   \n",
       "...                                                                                                                                              ...   \n",
       "1506     See Aug 4 2015 PoconoRecord @EmergencyMgtMag - How Many Households Have an #Emergency Plan? | http://t.co/7zlsUmIess http://t.co/TdccH01N7q   \n",
       "1511                                                        Pandemonium In Aba As Woman Delivers Baby Without Face (Photos) - http://t.co/xRP0rTkFfJ   \n",
       "1512                             #volleyball Attack II Volleyball Training Machine - Sets Simulation - http://t.co/dCDeCFv934 http://t.co/dWBC1dUvdk   \n",
       "1513      kesabaran membuahkan hasil indah pada saat tepat! life isn't about waiting for the storm to pass it's about learning to dance in the rain.   \n",
       "1522                                           I moved to England five years ago today. What a whirlwind of time it has been! http://t.co/eaSlGeA1B7   \n",
       "\n",
       "                                                                                                                               processed_tweet  \\\n",
       "5       <user> one good thing came out of watching the film was too traumatised to watch show so started halt amp catch fire on amazon <smile>   \n",
       "22         gonna call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year   \n",
       "38                                                     <user> it s the closest structure to the hypo centre that wasn t completely obliterated   \n",
       "55                         oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died   \n",
       "72               bigamist and his first wife are charged in the deaths of his second pregnant wife her child <number> her mothe <repeat> <url>   \n",
       "...                                                                                                                                        ...   \n",
       "1506                    see aug <number> <number> poconorecord <user> how many households have an <hashtag> <elong> emergency plan <url> <url>   \n",
       "1511                                                                       pandemonium in aba as woman delivers baby without face photos <url>   \n",
       "1512                                          <hashtag> volleyball attack ii <allcaps> volleyball training machine sets simulation <url> <url>   \n",
       "1513  kesabaran membuahkan hasil indah pada saat tepat life isn t about waiting for the storm to pass it s about learning to dance in the rain   \n",
       "1522                                                        i moved to england five years ago today what a whirlwind of time it has been <url>   \n",
       "\n",
       "      true_label  prediction  \n",
       "5              1           0  \n",
       "22             1           0  \n",
       "38             1           0  \n",
       "55             1           0  \n",
       "72             1           0  \n",
       "...          ...         ...  \n",
       "1506           1           0  \n",
       "1511           1           0  \n",
       "1512           1           0  \n",
       "1513           1           0  \n",
       "1522           1           0  \n",
       "\n",
       "[192 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1cb8d",
   "metadata": {},
   "source": [
    "Different model architectures:\n",
    "* Convoluational NN - try a window of words from the training example. Window size of 4/5 - pick out top 5 words of TFIDF\n",
    "* Recurrent NN - feed words in 1 at a time, allow network to persist state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c338c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
